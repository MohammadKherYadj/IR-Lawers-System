{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ls_7d4uCL_36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading punkt_tab: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10013]\n",
      "[nltk_data]     An attempt was made to access a socket in a way\n",
      "[nltk_data]     forbidden by its access permissions>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "# from textblob import TextBlob\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsW5RqkWMB_o"
   },
   "outputs": [],
   "source": [
    "class Preprocessing_EN:\n",
    "  @staticmethod\n",
    "  def process(sentance):\n",
    "    sentance = Preprocessing_EN.remove_punctuation(sentance)\n",
    "    sentance = Preprocessing_EN.tokenizer(sentance)\n",
    "    sentance = Preprocessing_EN.normalizer(sentance)\n",
    "    sentance = Preprocessing_EN.remove_stopwords(sentance)\n",
    "    sentance = Preprocessing_EN.remove_deplicate(sentance)\n",
    "    sentance = Preprocessing_EN.stemmer(sentance)\n",
    "    # sentance = Preprocessing_EN.lemmatizer(sentance)\n",
    "    \n",
    "    return sentance\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer(sentance):\n",
    "\n",
    "    words = word_tokenize(sentance)\n",
    "    return words\n",
    "\n",
    "  @staticmethod\n",
    "  def normalizer(sentance):\n",
    "    return [word.lower() for word in sentance]\n",
    "\n",
    "  @staticmethod\n",
    "  def remove_stopwords(sentance):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    sentance = [word for word in sentance if word not in stop_words]\n",
    "    return sentance\n",
    "\n",
    "  @staticmethod\n",
    "  def lemmatizer(sentance):\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    sentance = [lemmatize.lemmatize(word) for word in sentance]\n",
    "    return sentance\n",
    "  \n",
    "  @staticmethod\n",
    "  def stemmer(sentance):\n",
    "    stemmer = PorterStemmer()\n",
    "    sentance = [stemmer.stem(word) for word in sentance]\n",
    "    return sentance\n",
    "\n",
    "  @staticmethod\n",
    "  def remove_punctuation(sentance):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]',' ',sentance)\n",
    "\n",
    "  @staticmethod\n",
    "  def remove_deplicate(sentance):\n",
    "    return list(set(sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NofI0Q6XNL3r",
    "outputId": "1fec5c65-6f23-4650-cea7-4605ce9e5b25"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accur',\n",
       " 'globe',\n",
       " 'industri',\n",
       " 'artifici',\n",
       " 'across',\n",
       " 'faster',\n",
       " 'busi',\n",
       " 'make',\n",
       " 'mainstream',\n",
       " 'learn',\n",
       " 'healthcar',\n",
       " 'applic',\n",
       " 'financ',\n",
       " 'expand',\n",
       " 'process',\n",
       " 'process',\n",
       " 'natur',\n",
       " 'capabl',\n",
       " 'power',\n",
       " 'languag',\n",
       " 'transform',\n",
       " 'continu',\n",
       " 'ai',\n",
       " 'intellig',\n",
       " 'solut',\n",
       " 'machin']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Artificial Intelligence (AI) is transforming industries across the globe. From healthcare to finance,\n",
    "AI-powered solutions are making processes faster and more accurate. AI applications, such as natural\n",
    "language processing and machine learning, are now mainstream in businesses, and their capabilities continue\n",
    "to expand.\"\"\"\n",
    "\n",
    "text1 = \"\"\"Rocks Making Swimming Saw Was watched see\"\"\"\n",
    "\n",
    "Preprocessing_EN.process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_AR:\n",
    "  @staticmethod\n",
    "  def process(sentance):\n",
    "    sentance = Preprocessing_AR.remove_punctuation(sentance)\n",
    "    sentance = Preprocessing_AR.normalizer(sentance)\n",
    "    sentance = Preprocessing_AR.tokenizer(sentance)\n",
    "    sentance = Preprocessing_AR.remove_stopwords(sentance)\n",
    "    sentance = Preprocessing_AR.remove_deplicate(sentance)\n",
    "    sentance = Preprocessing_AR.stemmer(sentance)\n",
    "    \n",
    "    return sentance\n",
    "\n",
    "  @staticmethod\n",
    "  def tokenizer(sentance):\n",
    "    words = word_tokenize(sentance)\n",
    "    return words\n",
    "\n",
    "  @staticmethod\n",
    "  def normalizer(sentance):\n",
    "    sentance = re.sub(r\"[إأٱآا]\",\"ا\",sentance)\n",
    "    sentance = re.sub(r\"ي\",\"ى\",sentance)\n",
    "    sentance = re.sub(r\"ء\",\"ؤ\",sentance)\n",
    "    sentance = re.sub(r\"ء\",\"ئ\",sentance)\n",
    "    sentance = re.sub(r\"[^ا-ي]\",\" \",sentance)\n",
    "    sentance = re.sub(r\"ة\",\"ه\",sentance)\n",
    "    return sentance\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def remove_stopwords(sentance):\n",
    "    \n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "\n",
    "    sentance = [word for word in sentance if word not in stop_words]\n",
    "    return sentance\n",
    "\n",
    "  @staticmethod\n",
    "  def lemmatizer(sentance):\n",
    "    lemmatize = WordNetLemmatizer()\n",
    "    sentance = [lemmatize.lemmatize(word) for word in sentance]\n",
    "    return sentance\n",
    "  \n",
    "  @staticmethod\n",
    "  def stemmer(sentance):\n",
    "    stemmer = PorterStemmer()\n",
    "    sentance = [stemmer.stem(word) for word in sentance]\n",
    "    return sentance\n",
    "\n",
    "  @staticmethod\n",
    "  def remove_punctuation(sentance):\n",
    "    return re.sub(r'[^ا-ي 0-9\\s]',' ',sentance)\n",
    "\n",
    "  @staticmethod\n",
    "  def remove_deplicate(sentance):\n",
    "    return list(set(sentance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['بىنهم', 'معقده', 'عقارىه', 'فىما', 'جمىع', 'طراف', 'متنازعىن', 'قضىه', 'ال']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"قضية عقارية معقدة جميع الأطراف متنازعين فيما بينهم\"\n",
    "Preprocessing_AR.process(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
